{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "MMAI5000_assignment3 -- Shaoshi Zhang 217875360.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ygZgZ0_yTLi"
      },
      "source": [
        "# AI Fundamentals - Assignment 3\n",
        "\n",
        "This assignment requires you to use [Tensorflow](https://www.tensorflow.org) and [Keras](https://keras.io/). Keras is a high-level Deep Learning API written in Python working as an interface to TensorFlow.\n",
        "\n",
        "This assignment is divided in two parts. In the first part you will learn about Keras with the help of the example below and the Keras [documentation](https://keras.io/). In the second part, you will practise training a Deep Learning model.\n",
        "\n",
        "## How to submit\n",
        "Submit by uploading this notebook to Canvas. It should include **plots**, **results** and **code** showing how the results were genereated.  Remember to name your file(s) appropriately.\n",
        "It is due on 11:59 of December 9, 2020.\n",
        "\n",
        "## Installation\n",
        "Instructions can be found here:\n",
        "* [Tensorflow](https://www.tensorflow.org/install/)\n",
        "\n",
        "Since Tensorflow 2.0, Keras is included in Tensorflow and will be automatically installed with Tensorflow. It can be accessed as ```tensorflow.keras```\n",
        "\n",
        "I recommend using ```pip```. For Tensorflow is it sufficient to install the CPU version. The GPU version requires a good workstation with high-end Nvidia GPU(s), and it is not necessary for this tutorial.\n",
        "\n",
        "If you're using a virtualenv:\n",
        "```\n",
        "pip3 install tensorflow\n",
        "```\n",
        "Add ```sudo``` for a systemwide installation (i.e. no ```virtualenv```).\n",
        "```\n",
        "sudo pip3 install tensorflow\n",
        "```\n",
        "Make sure that you have ```sklearn```, ```matplotlib``` and ```numpy``` installed, too.\n",
        "\n",
        "\n",
        "## Part 1 - understand a model\n",
        "\n",
        "### Optimizers\n",
        "\n",
        "Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater than zero. The goal of training a model is to find a set of weights and biases (i.e. parameters) that have, on average, a low loss across all examples. The term cost is used interchangably with loss. See the [loss section](https://keras.io/losses/) in the Keras documentation for a list and descriptions of what is available.\n",
        "\n",
        "![Side by side loss](https://drive.google.com/uc?id=1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae)\n",
        "<b>Figure 1.</b> Left: high loss and right: low loss.\n",
        "\n",
        "<!-- https://drive.google.com/file/d/1DdbQEQLCLCSw4uPsuf0C1nJCfUICT0Ae/view?usp=sharing\n",
        "<img src=\"./fig/LossSideBySide.png\" width=\"500\">\n",
        "<figcaption>Figure. Left: high loss and right: low loss.</figcaption>\n",
        " -->\n",
        "The optimizer is the algorithm used to minimize the loss/cost. Optimizers in neural networks work by finding the gradient/derivative of the loss with respect to the parameters (i.e. the weights). \"Gradient\" is the correct term since a we are looking at multi-dimensional systems (i.e. many parameters), however, the terms are often used interchangably. For those who didn't take multivariate calculus, just think of the gradient as a derivative. The derivative of the loss with respect to a parameters tells us how much the loss changes when we nudge a weight up or down. So, by knowing how a given parameter affects the loss the optimizer can change it so as to decrease the loss. The various optimizers differ in how they change the weights. \n",
        "\n",
        "#### Mini-overview over popular optimizers\n",
        "\n",
        "* **Stochastic Gradient Descent (SGD)**. This is the most basic and easy to understand optimizer. It updates the weights in the negative direction of the gradient by taking the average gradient of mini-batch of data (e.g. 20-1000 examples) in each step. Vanilla SGD only has one hyper-parameter, the learning rate.\n",
        "* **Momentum**. This optimizer \"gains speed\" when the gradient has pointed in the same direction for several consecutive updates. That is, it has a momentum and want to keep moving in that direction. It gains momentum by accumulating an exponentially decaying moving average of past gradients. The step size depends on how large and aligned the sequence of gradients are. The most important hyper-parameter is alpha and common values are 0.5 and 0.9.\n",
        "* **Nesterov Momentum**. This is a modification of the standard momentum optimizer.\n",
        "* **AdaGrad**. This optimizer Ada-ptively sets the learning rate depending on the steepness/magnitude of the Grad-ients. This is done so that weights with big gradients get a smaller effective learning rate, and weights with small gradients will get a greater effective learning rate. The result is quicker progress in the more gently sloped directions of the weight space and a slowdown in stepp regions.\n",
        "* **RMSProp**. This is modification of AdaGrad, where the accumulated gradient decays, that is, the influence of previous gradients gradually decreases.\n",
        "* **Adam**. The name comes from \"adaptive moments\", and it is a combination of RMSProp and momentum. It has several hyper-parameters.\n",
        "\n",
        "The above list just gives a quick overview of some of the most common. However, old optimizers are constantly improved and new are developed. SGD and momentum are most basic and easiest to understand and implement. They are still in use, but the more advanced optimizers tend to be better for practical use. Which one to use is generally an emperical question depending on both the data and the model.\n",
        "\n",
        "For a more complete overview of optimization algorithms see [this comparison](http://ruder.io/optimizing-gradient-descent/), and to see what is available in Keras, see the [optimizer section](https://keras.io/optimizers/) of the documentation.\n",
        "\n",
        "See the images below for a comparison of optimizers in a 2D space (NAG: Nesterov accelerated gradient, Adadelta: an extension of AdaGrad).\n",
        "\n",
        "![Contours - optimizer comparison](https://drive.google.com/uc?id=1CmrD-UPZ7EIUjRuO_ib7k9CL1FO2bbLk)\n",
        "<b>Figure 2.</b> Comparison of six different optimizers.\n",
        "\n",
        "\n",
        "![Saddle point - optimizer comparison](https://drive.google.com/uc?id=1QVhN9rAvCjXtGyNZkmFivyyCzNsntObh)\n",
        "<b>Figure 3.</b> Comparison of six different optimizers at a saddle point.\n",
        "\n",
        "<!-- <img src=\"./fig/contours_evaluation_optimizers.gif\" width=\"500\">\n",
        "<img src=\"./fig/saddle_point_evaluation_optimizers.gif\" width=\"500\"> -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PBB2nJhyTLj"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "# for the random seed\n",
        "import tensorflow as tf\n",
        "\n",
        "# set the random seeds to get reproducible results\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(2)\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X, y = X[:1000], y[:1000]\n",
        "X = X.reshape(X.shape[0], 28, 28, 1)\n",
        "# Normalize\n",
        "X = X / 255.\n",
        "# number of unique classes\n",
        "num_classes = len(np.unique(y))\n",
        "y = y.astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
        "\n",
        "num_tot = y.shape[0]\n",
        "num_train = y_train.shape[0]\n",
        "num_test = y_test.shape[0]\n",
        "\n",
        "y_oh = np.zeros((num_tot, num_classes))\n",
        "y_oh[range(num_tot), y] = 1\n",
        "\n",
        "y_oh_train = np.zeros((num_train, num_classes))\n",
        "y_oh_train[range(num_train), y_train] = 1\n",
        "\n",
        "y_oh_test = np.zeros((num_test, num_classes))\n",
        "y_oh_test[range(num_test), y_test] = 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TivQcmLgKP29",
        "outputId": "ca10e0fa-a3e5-4418-dbb1-6b08958f8eae"
      },
      "source": [
        "type(X)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_xJQ18rK66M",
        "outputId": "8c60914e-d524-41c1-9131-bc836b8afeee"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8saaRQJKTVx",
        "outputId": "2d9c6d22-82b5-45c2-d313-e77f2f9eceff"
      },
      "source": [
        "type(y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owj8MewzK9bl",
        "outputId": "1d62939f-24dd-4e22-f5c6-0e18dd67ceca"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQpmdv68xyat",
        "outputId": "52d4a885-a980-4458-af96-7bdacd960795"
      },
      "source": [
        "ex_index = np.random.randint(y_train.shape[0], size=3)\n",
        "ex_index"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 37, 235,  72])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1pHX-2JPrRx",
        "outputId": "63ad8481-3de7-40b9-dfaa-e59effc12cb5"
      },
      "source": [
        "y_oh_ex = y_oh[ex_index]\n",
        "y_oh_ex"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6XZAME5PyJV",
        "outputId": "07bf9401-9ad0-4ef5-f68f-7e5fcd2194c3"
      },
      "source": [
        "y_ex = y[ex_index]\n",
        "y_ex"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJTKbHFvyo3A",
        "outputId": "e330cb40-d70b-44de-9ff8-6ed01dd8d01a"
      },
      "source": [
        "X_train[ex_index][0].shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "ETFeQxhY7yE-",
        "outputId": "f2b4d241-cba3-44bc-d29b-4d95d2325678"
      },
      "source": [
        "X_ex = X_train[ex_index]\n",
        "X_ex = X_ex.reshape(X_ex.shape[0],28,28)\n",
        "\n",
        "for i in range(3):\n",
        "  plt.title('Label is {label}'.format(label=y_train[ex_index][i]))\n",
        "  fig = plt.figure\n",
        "  plt.imshow(X_ex[i], cmap='gray')\n",
        "  plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7tSTLUwyTLj"
      },
      "source": [
        "### Question 1\n",
        "**The data set**\n",
        "\n",
        "Plot a three examples from the data set.\n",
        "\n",
        "* What type of data are in the data set?\n",
        "\n",
        "    ANS: Both X and y are numpy.ndarray but they have different shapes that X is in shape(1000,28,28,1) where y is in shape(1000,). That means X is a 4D array but y is a 1D array. \n",
        "    \n",
        "\n",
        "* What does the line ```X = X.reshape(X.shape[0], 28, 28, 1)``` do?\n",
        "\n",
        "    ANS: The original dataset has 70000 rows and 784 features. Here we take the first 1000 rows as our dataset in this assignment. Before the reshape, X has a shape of (1000,784) which means 1000 rows and 784 columns. The reshape command here is to transform X dataset to a dataset that contains 1000 images(rows), each of size 28*28 pixels(28^2=784 columns). And since they are grayscale images, the \"1\" is an empty dimension to match the input shape of the neural network.\n",
        "\n",
        "Look at how the encoding of the targets (i.e. ```y```) is changed. E.g. the lines\n",
        "```\n",
        "    y_oh = np.zeros((num_tot, num_classes))\n",
        "    y_oh[range(num_tot), y] = 1\n",
        "```\n",
        "Print out a few rows of ```y``` next to ```y_oh```.\n",
        "\n",
        "\n",
        "* What is the relationship between ```y``` and ```y_oh```?\n",
        "\n",
        "    ANS: y_oh is a 2D array whose length is the same as y, 1000. But y_oh has 10 columns representing y has 10 unique values. First we create a 2D array with all values are 0 in shape of (1000,10). Then we assign 1s to the 2D array according to the exact value of y. For example, if the 101th value of y is 6, then we fill the y_oh[100,5] with the number of 1. Therefore, we transformed the value of 6 to a expression of 0000010000. In this way, we made each entry of y become a binary series with length of 10. And we made a copy of y in catagorical variables instead of different integers. \n",
        "    \n",
        "    \n",
        "* What is the type of encoding in ```y_oh``` called and why is it used?\n",
        "\n",
        "    ANS: This is called one-hot encoding. This is used because we want to take the value of y into catagorical variables to make ML algorithms to do a better job in prediction. The one hot encoding allows can convert categorical data into numbers, and these numbers are not ordered value, they are the symbols of catagories. This is required for both input and output variables that are categorical. When a one hot encoding is used for the output variable, it may offer a more nuanced set of predictions than a single label, and therefore, we can estimate the MSE loss correspondingly.\n",
        "    \n",
        "    \n",
        "* Plot three data examples in the same figure and set the correct label as title. \n",
        "    * It should be possible to see what the data represent.\n",
        "\n",
        "    ANS: Plots are shown as output above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvpZJZMWyTLj"
      },
      "source": [
        "### Question 2\n",
        "**The model**\n",
        "\n",
        "Below is some code for bulding and training a model with Keras.\n",
        "* What type of network is implemented below? I.e. a normal MLP, RNN, CNN, Logistic Regression...?\n",
        "\n",
        "    ANS : This is a Convolutional Neural Network (CNN), maybe a multi-layer CNN since there are two layers of ReLu.\n",
        "    \n",
        "    \n",
        "* What does ```Dropout()``` do?\n",
        "\n",
        "    ANS: The dropout function here allows us to ignore a certain proportion of units in the CNN network. By dropping a unit out, we temporarily ignore it from the network, along with all its incoming and outgoing (hidden and visible) connections. The number specified in the Dropout() function is the fraction of the input units to drop.\n",
        "\n",
        "    Dropout function allow us to learn a fraction of the inputs in the network in each training iteration, instead of learning all the inputs together. Therefore, we can prevent the co-dependency between parameters developed during training, and we can prevent our fitted model from overfitting. \n",
        "\n",
        "\n",
        "* Which type of activation function is used for the hidden layers?\n",
        "\n",
        "    ANS : ReLu function is used for the hidden layers.\n",
        "\n",
        "\n",
        "* Which type of activation function is used for the output layer?\n",
        "\n",
        "    ANS: Softmax function is used for the output layer.\n",
        "\n",
        "\n",
        "* Why are two different activation functions used?\n",
        "\n",
        "    ANS : Because ReLu and Softmax have different advantages and uses. \n",
        "    \n",
        "    ReLu function could train the network quickly because the gradient of ReLu is always high compared to other activation functions so that ReLu allows the network to converge very quickly. However, the Softmax function is totally different with ReLu. Softmax function is often used in the output layer because Softmax is able to normalizes the outputs for each catagory between 0 and 1, and divides by their sum, giving the probability of the input value being in a specific catagotry.\n",
        "\n",
        "    To summarize, ReLu is used to train the model quickly while Softmax is to provide probability of a input belonging to a specific catagory.\n",
        "\n",
        "\n",
        "\n",
        "* What optimizer is used in the model below?\n",
        "\n",
        "    ANS : Stochastic gradient descent is used as the optimizer here.\n",
        "\n",
        "\n",
        "* How often are the weights updated (i.e. after how many data examples)?\n",
        "\n",
        "    ANS: We can see that we set the batch size to 32. And A batch size of 32 means that 32 samples from the training dataset will be used to estimate the error gradient before the model weights are updated. \n",
        "\n",
        "    Therefore, the weights will be updated after every 32 data examples.\n",
        "\n",
        "\n",
        "* What loss function is used?\n",
        "\n",
        "    ANS : Categorical Crossentropy loss function is used since we have 10 different label catagories. This loss function allows us to compute the crossentropy loss between the labels and predictions. \n",
        "\n",
        "\n",
        "* How many parameters (i.e. weights and biases, NOT hyper-parameters) does the model have?\n",
        "\n",
        "    ANS : All Max-pooling functions do not require any parameters because it is a mathematical operation to find the maximum.\n",
        "\n",
        "    The first Conv2D layer has 16 * (3 * 3+1) = 160 parameters. The second Conv2d layer has (3 * 3 * 16+1) * 32 = 4640 parameters. The third layer is a dense layer, from the output of model.summary() we can see that the third layer has (800+1) * 128 = 102528 parameters. And the last layer is a Softmax layer which has (128 * 10 +10) = 1290 parameters. Full architecture of the model is shown with the model.summary() command.\n",
        "\n",
        "    Therefore, this model has 160 + 4640 + 102528 + 1290 = 108618 parameters. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6gUi_8INyTLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ecb9594-0882-40cb-e93b-7599951f3daf"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_oh_train, batch_size=32, epochs=60)\n",
        "\n",
        "# Evaluate performance\n",
        "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
        "\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "25/25 [==============================] - 1s 19ms/step - loss: 2.2322\n",
            "Epoch 2/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 1.3094\n",
            "Epoch 3/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.5476\n",
            "Epoch 4/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.3458\n",
            "Epoch 5/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.2481\n",
            "Epoch 6/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.1618\n",
            "Epoch 7/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.1702\n",
            "Epoch 8/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.1203\n",
            "Epoch 9/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0991\n",
            "Epoch 10/60\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0544\n",
            "Epoch 11/60\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0416\n",
            "Epoch 12/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.0405\n",
            "Epoch 13/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.0346\n",
            "Epoch 14/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0197\n",
            "Epoch 15/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0155\n",
            "Epoch 16/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0179\n",
            "Epoch 17/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0119\n",
            "Epoch 18/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0071\n",
            "Epoch 19/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0077\n",
            "Epoch 20/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.0040\n",
            "Epoch 21/60\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0042\n",
            "Epoch 22/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.0032\n",
            "Epoch 23/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0035\n",
            "Epoch 24/60\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0023\n",
            "Epoch 25/60\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0022\n",
            "Epoch 26/60\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0022\n",
            "Epoch 27/60\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0021\n",
            "Epoch 28/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0019\n",
            "Epoch 29/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0019\n",
            "Epoch 30/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0015\n",
            "Epoch 31/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0018\n",
            "Epoch 32/60\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0015\n",
            "Epoch 33/60\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0013\n",
            "Epoch 34/60\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0013\n",
            "Epoch 35/60\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0012\n",
            "Epoch 36/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.0011\n",
            "Epoch 37/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.0012\n",
            "Epoch 38/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 0.0011\n",
            "Epoch 39/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0012\n",
            "Epoch 40/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 0.0011\n",
            "Epoch 41/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 0.0010\n",
            "Epoch 42/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 9.5389e-04\n",
            "Epoch 43/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 8.7589e-04\n",
            "Epoch 44/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 7.5664e-04\n",
            "Epoch 45/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 7.8364e-04\n",
            "Epoch 46/60\n",
            "25/25 [==============================] - 1s 20ms/step - loss: 8.6218e-04\n",
            "Epoch 47/60\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 7.4382e-04\n",
            "Epoch 48/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 5.9547e-04\n",
            "Epoch 49/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 7.4040e-04\n",
            "Epoch 50/60\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 6.5876e-04\n",
            "Epoch 51/60\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.1762e-04\n",
            "Epoch 52/60\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.8019e-04\n",
            "Epoch 53/60\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.5301e-04\n",
            "Epoch 54/60\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.6151e-04\n",
            "Epoch 55/60\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 6.9891e-04\n",
            "Epoch 56/60\n",
            "25/25 [==============================] - 0s 19ms/step - loss: 6.1614e-04\n",
            "Epoch 57/60\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.2972e-04\n",
            "Epoch 58/60\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 6.1560e-04\n",
            "Epoch 59/60\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 5.8387e-04\n",
            "Epoch 60/60\n",
            "25/25 [==============================] - 0s 20ms/step - loss: 5.5804e-04\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.4677\n",
            "Accuracy: 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miipu4ip30Xs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa43974c-4475-459d-a957-2a6010969e45"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 16)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 13, 13, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 108,618\n",
            "Trainable params: 108,618\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os-gzwN3yTLk"
      },
      "source": [
        "## Part 2 - train a model\n",
        "\n",
        "A model's performance depends on many factors apart from the model architecture (e.g. type and number of layers) and the dataset. Here you will get to explore some of the factors that affect model performance. Much of the skill in training deep learning models lies in quickly finding good values/options for these choises.\n",
        "\n",
        "In order to observe the learning process it is best to compare the training set loss with the loss on the test set. How to visualize these variables with Keras is described under [Training history visualization](https://keras.io/visualization/#training-history-visualization) in the documentation.\n",
        "\n",
        "You will explore the effect of 1) optimizer, 2) training duration, and 3) dropout (see the question above).\n",
        "\n",
        "When training, an **epoch** is one pass through the full training set.\n",
        "\n",
        "### Question 3\n",
        "\n",
        "* **Vizualize the training**. Use the model above to observe the training process. Train it for 150 epochs and then plot both \"loss\" and \"val_loss\" (i.e. loss on the valiadtion set, here the terms \"validation set\" and \"test set\" are used interchangably, but this is not always true). What is the optimal number of epochs for minimizing the test set loss? \n",
        "    * Remember to first reset the weights (```model.reset_states()```), otherwise the training just continues from where it was stopped earlier.\n",
        "\n",
        "\n",
        "* ANS: Below are the codes I run to visualize the loss and val_loss. Since the model.rest_state() does not work and model.compile() does not work either, I re-set the entire model each time. In the following code I train the model for 150 epochs. And from the plot in the output, we can see that the val_loss is approximately decreasing in the first 20 epochs and then the val_loss is steady increasing after 25 ~ 30 epochs. \n",
        "\n",
        "  Therefore, I would say the at least 30 epochs are required for the best performance(lowest val_loss). However I would use 30+20=50 epochs in the following parts to do the GridSearch for the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_P7XZ6d2Kmz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "085b666f-d730-4d30-8fc7-846f07251ffc"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "# Max pooling\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_oh_train, batch_size=32, epochs=150, validation_data=(X_test, y_oh_test))\n",
        "\n",
        "# Evaluate performance\n",
        "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\n",
        "\n",
        "predictions = model.predict(X_test, batch_size=32)\n",
        "predictions = np.argmax(predictions, axis=1) # change encoding again\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])\n",
        "\n",
        "# Plot learning\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "ax0 = fig.add_subplot(121)\n",
        "ax0.plot(history.history['val_loss'], label='validation loss')\n",
        "ax0.plot(history.history['loss'], label='training loss')\n",
        "ax0.set_ylabel('Loss')\n",
        "ax0.set_xlabel('Epoch')\n",
        "ax0.legend()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "25/25 [==============================] - 1s 30ms/step - loss: 2.2317 - val_loss: 1.7812\n",
            "Epoch 2/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 1.2515 - val_loss: 0.7918\n",
            "Epoch 3/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.5384 - val_loss: 0.4853\n",
            "Epoch 4/150\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 0.3447 - val_loss: 0.3674\n",
            "Epoch 5/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.2615 - val_loss: 0.4285\n",
            "Epoch 6/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.2047 - val_loss: 0.3760\n",
            "Epoch 7/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1651 - val_loss: 0.3550\n",
            "Epoch 8/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.1485 - val_loss: 0.3529\n",
            "Epoch 9/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.0979 - val_loss: 0.3083\n",
            "Epoch 10/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.0642 - val_loss: 0.3670\n",
            "Epoch 11/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0480 - val_loss: 0.3779\n",
            "Epoch 12/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0588 - val_loss: 0.3347\n",
            "Epoch 13/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0394 - val_loss: 0.3355\n",
            "Epoch 14/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0254 - val_loss: 0.3773\n",
            "Epoch 15/150\n",
            "25/25 [==============================] - 1s 21ms/step - loss: 0.0252 - val_loss: 0.3683\n",
            "Epoch 16/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0289 - val_loss: 0.3750\n",
            "Epoch 17/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.0172 - val_loss: 0.3083\n",
            "Epoch 18/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.0071 - val_loss: 0.3438\n",
            "Epoch 19/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.0071 - val_loss: 0.3501\n",
            "Epoch 20/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0047 - val_loss: 0.3726\n",
            "Epoch 21/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0037 - val_loss: 0.3642\n",
            "Epoch 22/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.0030 - val_loss: 0.3717\n",
            "Epoch 23/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0029 - val_loss: 0.3664\n",
            "Epoch 24/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0021 - val_loss: 0.3806\n",
            "Epoch 25/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0021 - val_loss: 0.3783\n",
            "Epoch 26/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0021 - val_loss: 0.3848\n",
            "Epoch 27/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.0019 - val_loss: 0.3901\n",
            "Epoch 28/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.0017 - val_loss: 0.3885\n",
            "Epoch 29/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0017 - val_loss: 0.3952\n",
            "Epoch 30/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0014 - val_loss: 0.3944\n",
            "Epoch 31/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.3976\n",
            "Epoch 32/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.0014 - val_loss: 0.4037\n",
            "Epoch 33/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.0012 - val_loss: 0.4005\n",
            "Epoch 34/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0012 - val_loss: 0.4063\n",
            "Epoch 35/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0012 - val_loss: 0.4084\n",
            "Epoch 36/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 9.8031e-04 - val_loss: 0.4090\n",
            "Epoch 37/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0011 - val_loss: 0.4141\n",
            "Epoch 38/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0011 - val_loss: 0.4146\n",
            "Epoch 39/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.0011 - val_loss: 0.4188\n",
            "Epoch 40/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 9.5357e-04 - val_loss: 0.4164\n",
            "Epoch 41/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 9.2746e-04 - val_loss: 0.4199\n",
            "Epoch 42/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 8.8658e-04 - val_loss: 0.4209\n",
            "Epoch 43/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 8.1601e-04 - val_loss: 0.4244\n",
            "Epoch 44/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 6.9085e-04 - val_loss: 0.4255\n",
            "Epoch 45/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 6.8848e-04 - val_loss: 0.4255\n",
            "Epoch 46/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 8.0820e-04 - val_loss: 0.4287\n",
            "Epoch 47/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 6.6913e-04 - val_loss: 0.4314\n",
            "Epoch 48/150\n",
            "25/25 [==============================] - 1s 34ms/step - loss: 5.9091e-04 - val_loss: 0.4321\n",
            "Epoch 49/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 7.0046e-04 - val_loss: 0.4317\n",
            "Epoch 50/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 6.2281e-04 - val_loss: 0.4357\n",
            "Epoch 51/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.8914e-04 - val_loss: 0.4352\n",
            "Epoch 52/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 5.2691e-04 - val_loss: 0.4361\n",
            "Epoch 53/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 5.2206e-04 - val_loss: 0.4375\n",
            "Epoch 54/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 5.3382e-04 - val_loss: 0.4402\n",
            "Epoch 55/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 6.5266e-04 - val_loss: 0.4399\n",
            "Epoch 56/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 5.4927e-04 - val_loss: 0.4431\n",
            "Epoch 57/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 4.6083e-04 - val_loss: 0.4435\n",
            "Epoch 58/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 5.6716e-04 - val_loss: 0.4439\n",
            "Epoch 59/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 5.6154e-04 - val_loss: 0.4471\n",
            "Epoch 60/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 5.3198e-04 - val_loss: 0.4460\n",
            "Epoch 61/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 4.9546e-04 - val_loss: 0.4498\n",
            "Epoch 62/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 4.6048e-04 - val_loss: 0.4499\n",
            "Epoch 63/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 4.8227e-04 - val_loss: 0.4479\n",
            "Epoch 64/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 4.4597e-04 - val_loss: 0.4511\n",
            "Epoch 65/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 4.1749e-04 - val_loss: 0.4519\n",
            "Epoch 66/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 3.9417e-04 - val_loss: 0.4527\n",
            "Epoch 67/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 3.8451e-04 - val_loss: 0.4530\n",
            "Epoch 68/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.7447e-04 - val_loss: 0.4557\n",
            "Epoch 69/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 3.9265e-04 - val_loss: 0.4561\n",
            "Epoch 70/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 3.7356e-04 - val_loss: 0.4568\n",
            "Epoch 71/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 3.6184e-04 - val_loss: 0.4573\n",
            "Epoch 72/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 4.8039e-04 - val_loss: 0.4587\n",
            "Epoch 73/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 4.1381e-04 - val_loss: 0.4598\n",
            "Epoch 74/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.7948e-04 - val_loss: 0.4619\n",
            "Epoch 75/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.3218e-04 - val_loss: 0.4594\n",
            "Epoch 76/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.2240e-04 - val_loss: 0.4613\n",
            "Epoch 77/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 3.5311e-04 - val_loss: 0.4634\n",
            "Epoch 78/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.9094e-04 - val_loss: 0.4646\n",
            "Epoch 79/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 3.6342e-04 - val_loss: 0.4635\n",
            "Epoch 80/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 3.2788e-04 - val_loss: 0.4652\n",
            "Epoch 81/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 2.7355e-04 - val_loss: 0.4653\n",
            "Epoch 82/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 3.6827e-04 - val_loss: 0.4663\n",
            "Epoch 83/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 3.1238e-04 - val_loss: 0.4688\n",
            "Epoch 84/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 3.4938e-04 - val_loss: 0.4676\n",
            "Epoch 85/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.6110e-04 - val_loss: 0.4689\n",
            "Epoch 86/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 3.0831e-04 - val_loss: 0.4697\n",
            "Epoch 87/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 2.8078e-04 - val_loss: 0.4702\n",
            "Epoch 88/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 2.8954e-04 - val_loss: 0.4702\n",
            "Epoch 89/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.9907e-04 - val_loss: 0.4713\n",
            "Epoch 90/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.7487e-04 - val_loss: 0.4726\n",
            "Epoch 91/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.5759e-04 - val_loss: 0.4732\n",
            "Epoch 92/150\n",
            "25/25 [==============================] - 1s 31ms/step - loss: 2.9935e-04 - val_loss: 0.4733\n",
            "Epoch 93/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.7350e-04 - val_loss: 0.4748\n",
            "Epoch 94/150\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 2.5524e-04 - val_loss: 0.4748\n",
            "Epoch 95/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 2.8040e-04 - val_loss: 0.4757\n",
            "Epoch 96/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.2801e-04 - val_loss: 0.4775\n",
            "Epoch 97/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 2.6851e-04 - val_loss: 0.4760\n",
            "Epoch 98/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 2.3550e-04 - val_loss: 0.4780\n",
            "Epoch 99/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 2.3886e-04 - val_loss: 0.4783\n",
            "Epoch 100/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 2.2607e-04 - val_loss: 0.4796\n",
            "Epoch 101/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.3051e-04 - val_loss: 0.4787\n",
            "Epoch 102/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 2.8846e-04 - val_loss: 0.4800\n",
            "Epoch 103/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 2.6176e-04 - val_loss: 0.4809\n",
            "Epoch 104/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.7580e-04 - val_loss: 0.4814\n",
            "Epoch 105/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.5323e-04 - val_loss: 0.4818\n",
            "Epoch 106/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.9882e-04 - val_loss: 0.4838\n",
            "Epoch 107/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.4111e-04 - val_loss: 0.4832\n",
            "Epoch 108/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.5817e-04 - val_loss: 0.4833\n",
            "Epoch 109/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.2261e-04 - val_loss: 0.4832\n",
            "Epoch 110/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.1806e-04 - val_loss: 0.4857\n",
            "Epoch 111/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.5266e-04 - val_loss: 0.4848\n",
            "Epoch 112/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.1069e-04 - val_loss: 0.4859\n",
            "Epoch 113/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.8586e-04 - val_loss: 0.4865\n",
            "Epoch 114/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 2.1460e-04 - val_loss: 0.4869\n",
            "Epoch 115/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.8935e-04 - val_loss: 0.4874\n",
            "Epoch 116/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 2.2871e-04 - val_loss: 0.4880\n",
            "Epoch 117/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.8353e-04 - val_loss: 0.4886\n",
            "Epoch 118/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.2334e-04 - val_loss: 0.4891\n",
            "Epoch 119/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 1.8152e-04 - val_loss: 0.4897\n",
            "Epoch 120/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.7862e-04 - val_loss: 0.4898\n",
            "Epoch 121/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.7904e-04 - val_loss: 0.4907\n",
            "Epoch 122/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.9358e-04 - val_loss: 0.4913\n",
            "Epoch 123/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.0359e-04 - val_loss: 0.4904\n",
            "Epoch 124/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.2004e-04 - val_loss: 0.4924\n",
            "Epoch 125/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.6767e-04 - val_loss: 0.4934\n",
            "Epoch 126/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 2.0147e-04 - val_loss: 0.4930\n",
            "Epoch 127/150\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 1.6473e-04 - val_loss: 0.4927\n",
            "Epoch 128/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 1.7369e-04 - val_loss: 0.4932\n",
            "Epoch 129/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.8280e-04 - val_loss: 0.4940\n",
            "Epoch 130/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.6621e-04 - val_loss: 0.4943\n",
            "Epoch 131/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.6256e-04 - val_loss: 0.4955\n",
            "Epoch 132/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.8444e-04 - val_loss: 0.4957\n",
            "Epoch 133/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.6103e-04 - val_loss: 0.4966\n",
            "Epoch 134/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.7540e-04 - val_loss: 0.4958\n",
            "Epoch 135/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.7207e-04 - val_loss: 0.4970\n",
            "Epoch 136/150\n",
            "25/25 [==============================] - 1s 33ms/step - loss: 1.6454e-04 - val_loss: 0.4971\n",
            "Epoch 137/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.6876e-04 - val_loss: 0.4977\n",
            "Epoch 138/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.5186e-04 - val_loss: 0.4981\n",
            "Epoch 139/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.6753e-04 - val_loss: 0.4986\n",
            "Epoch 140/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.5816e-04 - val_loss: 0.4990\n",
            "Epoch 141/150\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 1.9293e-04 - val_loss: 0.5002\n",
            "Epoch 142/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.5653e-04 - val_loss: 0.5005\n",
            "Epoch 143/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.5908e-04 - val_loss: 0.5006\n",
            "Epoch 144/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.7403e-04 - val_loss: 0.5010\n",
            "Epoch 145/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.4901e-04 - val_loss: 0.5019\n",
            "Epoch 146/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.4301e-04 - val_loss: 0.5011\n",
            "Epoch 147/150\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 1.6392e-04 - val_loss: 0.5015\n",
            "Epoch 148/150\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 1.4850e-04 - val_loss: 0.5033\n",
            "Epoch 149/150\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 1.3602e-04 - val_loss: 0.5026\n",
            "Epoch 150/150\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 1.5112e-04 - val_loss: 0.5038\n",
            "7/7 [==============================] - 0s 7ms/step - loss: 0.5038\n",
            "Accuracy: 0.925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6c1c31f278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsqQ0RWEhb8K"
      },
      "source": [
        "\n",
        "* **Optimizer**. Select three different optimizers and for each find the close-to-optimal hyper-parameter(s). In your answer, include a) your three choises, b) best hyper-parameters for each of the three optimizers and, c) the code that produced the results.\n",
        "    * *NOTE* that how long the training takes varies with optimizer. I.e., make sure that the model is trained for long enough to reach optimal performance.\n",
        "\n",
        "    ANS : Here I used Adam, Nadam(NAG) and Adadelta as the three optimizers I chose. Below are the codes to use Grid Search to find the optimal hyper-parameters. All Grid Search are done based on 50 epochs.\n",
        "\n",
        "    Best hyper-parameters for Adam: beta_1 = 0.5, beta_2 = 0.8, learning_rate = 0.01. The final accuracy score is 0.931233\n",
        "    \n",
        "    Best hyper-parameters for Nadam(NAG): beta_1 = 0.9, beta_2 = 0.8, learning_rate = 1. The final accuracy score is 0.925000.\n",
        "\n",
        "    Best hyper-parameters for Adadelta: rho = 1, learning_rate = 0.1. The final accuracy score is 0.191218.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Htk0jsA-hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffefa828-15ab-4a38-b796-c415ad32b85c"
      },
      "source": [
        "# Use scikit-learn to grid search the hyperparameter for Adam\n",
        "import numpy \n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "Adam = Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False)\n",
        "Adadelta = Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07)\n",
        "NAG = Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(learning_rate=0.001, beta_1=0.9, beta_2=0.999):\n",
        "\t# create model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "  # Max pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "  # Max pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam, metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "# fix random seed for reproducibility\n",
        "seed = 1\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "# define the grid search parameters\n",
        "space = dict()\n",
        "space['learning_rate'] = [0.001, 0.01, 0.1,1]\n",
        "space['beta_1'] = [0.5,0.7,0.9]\n",
        "space['beta_2'] = [0.8,0.9,0.999]\n",
        "space['batch_size'] = [32]\n",
        "space['epochs']=[50]\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=space, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_oh_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.931233 using {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.913736 (0.015977) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.931233 (0.014550) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.917472 (0.020201) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.911243 (0.015731) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 1}\n",
            "0.919992 (0.026391) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.914984 (0.015187) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.912483 (0.019511) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.923728 (0.019765) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 1}\n",
            "0.916223 (0.020499) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.914975 (0.015563) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.922494 (0.015726) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.914993 (0.004793) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 1}\n",
            "0.914989 (0.009935) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.920001 (0.010729) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.912492 (0.008912) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.913726 (0.014189) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 1}\n",
            "0.908737 (0.014215) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.904992 (0.017435) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.914989 (0.009935) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.904987 (0.014218) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 1}\n",
            "0.917486 (0.021468) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.914993 (0.007143) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.913736 (0.013433) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.909986 (0.011156) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 1}\n",
            "0.921254 (0.005232) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.928745 (0.005364) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.924990 (0.010667) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.916242 (0.008909) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 1}\n",
            "0.916237 (0.011680) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.911234 (0.023224) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.913745 (0.003197) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.909995 (0.020075) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 1}\n",
            "0.909976 (0.023991) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.923733 (0.019501) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.919987 (0.009463) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.918748 (0.016851) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jJCR8CR0Y9p",
        "outputId": "576e792a-8e74-4ab3-db79-8eb9ac0e6418"
      },
      "source": [
        "# Use scikit-learn to grid search the hyperparameter of Nadam (NAG)\n",
        "import numpy \n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "Adam = Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False)\n",
        "Adadelta = Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07)\n",
        "NAG = Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(learning_rate=0.001, beta_1=0.9, beta_2=0.999):\n",
        "\t# create model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "  # Max pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "  # Max pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=NAG, metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "# fix random seed for reproducibility\n",
        "seed = 2\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "# define the grid search parameters\n",
        "space = dict()\n",
        "space['learning_rate'] = [0.001, 0.01, 0.1,1]\n",
        "space['beta_1'] = [0.5,0.7,0.9]\n",
        "space['beta_2'] = [0.8,0.9,0.999]\n",
        "space['batch_size'] = [32]\n",
        "space['epochs']=[50]\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=space, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_oh_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.925000 using {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 1}\n",
            "0.921250 (0.015291) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.907489 (0.023800) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.914998 (0.004702) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.921240 (0.010671) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 1}\n",
            "0.916251 (0.019891) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.920006 (0.006997) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.914984 (0.009000) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.911243 (0.018717) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 1}\n",
            "0.921240 (0.022089) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.909990 (0.019146) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.909990 (0.016239) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.919997 (0.015409) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 1}\n",
            "0.918739 (0.015151) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.922498 (0.010752) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.923742 (0.011634) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.912501 (0.019891) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 1}\n",
            "0.913750 (0.012233) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.915003 (0.006327) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.907498 (0.001854) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.912492 (0.011642) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 1}\n",
            "0.916237 (0.016913) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.916233 (0.014568) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.911253 (0.015375) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.916233 (0.016942) with: {'batch_size': 32, 'beta_1': 0.7, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 1}\n",
            "0.912483 (0.012500) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.911248 (0.019905) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.916233 (0.014568) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.925000 (0.018349) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.8, 'epochs': 50, 'learning_rate': 1}\n",
            "0.919978 (0.012899) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.913745 (0.017052) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.916233 (0.016942) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.912487 (0.022596) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.9, 'epochs': 50, 'learning_rate': 1}\n",
            "0.909990 (0.010681) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.001}\n",
            "0.922489 (0.012436) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.916247 (0.012382) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 0.1}\n",
            "0.922484 (0.020431) with: {'batch_size': 32, 'beta_1': 0.9, 'beta_2': 0.999, 'epochs': 50, 'learning_rate': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgIFvcwbjmgN",
        "outputId": "b286f9a6-2bb0-406e-b559-b5d20dfbe9af"
      },
      "source": [
        "# Use scikit-learn to grid search the hyperparameter of Adadelta \n",
        "import numpy \n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "Adam = Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07,amsgrad=False)\n",
        "Adadelta = Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07)\n",
        "NAG = Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(learning_rate=0.001, rho=0.95):\n",
        "\t# create model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "  # Max pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "  # Max pooling\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adadelta, metrics=[\"accuracy\"])\n",
        "\n",
        "  return model\n",
        "# fix random seed for reproducibility\n",
        "seed = 3\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "# define the grid search parameters\n",
        "space = dict()\n",
        "space['learning_rate'] = [0.001, 0.01, 0.1,1]\n",
        "space['rho'] = [0.001, 0.01, 0.1,1]\n",
        "space['batch_size'] = [32]\n",
        "space['epochs']=[50]\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=space, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_oh_train)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.191218 using {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.1, 'rho': 1}\n",
            "0.142547 (0.026802) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.001, 'rho': 0.001}\n",
            "0.166291 (0.024093) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.001, 'rho': 0.01}\n",
            "0.122483 (0.024832) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.001, 'rho': 0.1}\n",
            "0.145063 (0.036177) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.001, 'rho': 1}\n",
            "0.133682 (0.042832) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.01, 'rho': 0.001}\n",
            "0.162527 (0.035582) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.01, 'rho': 0.01}\n",
            "0.148752 (0.007731) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.01, 'rho': 0.1}\n",
            "0.152568 (0.039080) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.01, 'rho': 1}\n",
            "0.111238 (0.011469) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.1, 'rho': 0.001}\n",
            "0.167432 (0.055441) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.1, 'rho': 0.01}\n",
            "0.159913 (0.057994) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.1, 'rho': 0.1}\n",
            "0.191218 (0.018310) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 0.1, 'rho': 1}\n",
            "0.163715 (0.043172) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 1, 'rho': 0.001}\n",
            "0.139980 (0.025549) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 1, 'rho': 0.01}\n",
            "0.093774 (0.023995) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 1, 'rho': 0.1}\n",
            "0.158716 (0.058294) with: {'batch_size': 32, 'epochs': 50, 'learning_rate': 1, 'rho': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg2NxRLmA-6z"
      },
      "source": [
        "\n",
        "* **Dropout**. Use the best optimizer and do hyper-parameter seach and find the best value for ```Dropout()```.\n",
        "\n",
        "ANS: From  the above output we know that the Adam optimizer has the highest accuracy score of 0.931233 with beta_1 = 0.5, beta_2 = 0.8, learning_rate = 0.01. I do a Grid Search below to search for the optimal dropout rate.\n",
        "\n",
        "\n",
        "The best value for the dropout rate is 0.4 with the improved accuracy score of 0.934997 while keeping beta_1 = 0.5, beta_2 = 0.8 and learning_rate = 0.01 unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbhX-DZOmAE7",
        "outputId": "ca005f7b-8394-4b99-c5b9-0dab7453414b"
      },
      "source": [
        "# Use scikit-learn to grid search the dropout rate\r\n",
        "import numpy\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from keras.constraints import maxnorm\r\n",
        "\r\n",
        "# Function to create model, required for KerasClassifier\r\n",
        "def create_model(learning_rate=0.01, beta_1=0.5, beta_2=0.8, dropout_rate=0.):\r\n",
        "\t# create model\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\n",
        "  # Max pooling\r\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "  model.add(Dropout(0.))\r\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu'))\r\n",
        "  # Max pooling\r\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "  model.add(Flatten())\r\n",
        "  model.add(Dense(128, activation='relu'))\r\n",
        "  model.add(Dropout(0.))\r\n",
        "  model.add(Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "\r\n",
        "  # Compile the model\r\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=Adam, metrics=[\"accuracy\"])\r\n",
        "\r\n",
        "  return model\r\n",
        "\r\n",
        "# fix random seed for reproducibility\r\n",
        "seed = 4\r\n",
        "numpy.random.seed(seed)\r\n",
        "\r\n",
        "# create model\r\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\r\n",
        "\r\n",
        "# define the grid search parameters\r\n",
        "space = {}\r\n",
        "space['dropout_rate'] =  [0, 0.2, 0.4, 0.6, 0.8]\r\n",
        "space['learning_rate'] = [0.01]\r\n",
        "space['beta_1'] = [0.5]\r\n",
        "space['beta_2'] = [0.8]\r\n",
        "space['batch_size'] = [32]\r\n",
        "space['epochs']=[50]\r\n",
        "\r\n",
        "grid = GridSearchCV(estimator=model, param_grid=space, n_jobs=-1, cv=3)\r\n",
        "grid_result = grid.fit(X_train, y_oh_train)\r\n",
        "# summarize results\r\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\r\n",
        "means = grid_result.cv_results_['mean_test_score']\r\n",
        "stds = grid_result.cv_results_['std_test_score']\r\n",
        "params = grid_result.cv_results_['params']\r\n",
        "for mean, stdev, param in zip(means, stds, params):\r\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: 0.934997 using {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'dropout_rate': 0.4, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.929989 (0.009918) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'dropout_rate': 0, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.927501 (0.013782) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'dropout_rate': 0.2, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.934997 (0.009366) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'dropout_rate': 0.4, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.922484 (0.015179) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'dropout_rate': 0.6, 'epochs': 50, 'learning_rate': 0.01}\n",
            "0.919992 (0.014499) with: {'batch_size': 32, 'beta_1': 0.5, 'beta_2': 0.8, 'dropout_rate': 0.8, 'epochs': 50, 'learning_rate': 0.01}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-3kyxI1l_DP"
      },
      "source": [
        "* **Best model**. Combine the what you learned from the above three questions to build the best model. How much better is it than the worst and average models?\r\n",
        "\r\n",
        "    ANS : As we see from above, the best model is the one with Adam optimizer:  beta_1 = 0.5, beta_2 = 0.8 and learning_rate = 0.01 and dropout rate = 0.4. This model has the best accuracy score of 0.934997. \r\n",
        "\r\n",
        "    The worst one is the model with Adadelta whose accuracy score is 0.191218. The average of these models are (0.931233 + 0.925000 + 0.191218 + 0.934997)/4 = 0.74561125.\r\n",
        "\r\n",
        "    Therefore, the best model is about 75% better than the worst one and 20% better than the averages. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSJZN-1xlUmN"
      },
      "source": [
        "\r\n",
        "\r\n",
        "* **Results on the test set**. When doing this search for good model configuration/hyper-parameter values, the data set was split into *two* parts: a training set and a test set (the term \"validation\" was used interchangably wiht \"test\"). For your final model, is the performance (i.e. accuracy) on the test set representative for the performance one would expect on a previously unseen data set (drawn from the same distribution)? Why?\r\n",
        "\r\n",
        "    ANS: \r\n",
        "    \r\n",
        "    In the following code, I modified the model and plot the loss and the val_loss again with the hyperparameters of the best model where beta_1 = 0.5, beta_2 = 0.8 and learning_rate = 0.01 and dropout rate = 0.4. As we can see from the output below that the model accuracy on the test set is improved from 0.93 to 0.96. \r\n",
        "\r\n",
        "    However, this final best model may not work as well as expected on a previous unseen dataset, even the dataset is drawn from the same distribution. In previous questions, the hyperparameters are tuned with the training set data and the model are trained with training set. The \"best\" hyperparameters found by Grid Search are based on the model performance/accuracy on training set. Then, we selected the best final model solely based on the output of Grid Search. This means those \"best hyperparameters\" and the \"best model\" are the best for training set and they are not necesscary the best for the test/validation set. We should evaluate the model on the test/validation set with the tuned hyperparameters again each time whenever we did a Grid Search. And then, we should select the best model based on the best performance/accuracy on the test/validation set instead of on the training set. \r\n",
        "\r\n",
        "    From the loss and val_loss graph below we can see that the val_loss is very unstable while the loss of train set is steadly decreasing. \r\n",
        "\r\n",
        "    Therefore, in this case, the \"best final model\" has the risk of overfitting on the training set and it may not be significantly representative for the performance on a strange dataset. \r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jLMMtUJ-nWlc",
        "outputId": "627f858b-b46b-4651-eea5-899c03be83e4"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\r\n",
        "# Max pooling\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Dropout(0.4))\r\n",
        "\r\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\r\n",
        "# Max pooling\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Flatten())\r\n",
        "\r\n",
        "model.add(Dense(128, activation='relu'))\r\n",
        "model.add(Dropout(0.4))\r\n",
        "\r\n",
        "model.add(Dense(10, activation='softmax'))\r\n",
        "\r\n",
        "Adam = Adam(learning_rate=0.01,beta_1=0.5,beta_2=0.8,epsilon=1e-07,amsgrad=False)\r\n",
        "\r\n",
        "# Compile the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam)\r\n",
        "\r\n",
        "# Train the model\r\n",
        "history = model.fit(X_train, y_oh_train, batch_size=32, epochs=50, validation_data=(X_test, y_oh_test))\r\n",
        "\r\n",
        "# Evaluate performance\r\n",
        "test_loss = model.evaluate(X_test, y_oh_test, batch_size=32)\r\n",
        "\r\n",
        "predictions = model.predict(X_test, batch_size=32)\r\n",
        "predictions = np.argmax(predictions, axis=1) # change encoding again\r\n",
        "print('Accuracy:', (predictions == y_test).sum() / predictions.shape[0])\r\n",
        "\r\n",
        "# Plot learning\r\n",
        "fig = plt.figure(figsize=(12,4))\r\n",
        "ax0 = fig.add_subplot(121)\r\n",
        "ax0.plot(history.history['val_loss'], label='validation loss')\r\n",
        "ax0.plot(history.history['loss'], label='training loss')\r\n",
        "ax0.set_ylabel('Loss')\r\n",
        "ax0.set_xlabel('Epoch')\r\n",
        "ax0.legend()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "25/25 [==============================] - 1s 29ms/step - loss: 1.9620 - val_loss: 0.5396\n",
            "Epoch 2/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.5469 - val_loss: 0.4167\n",
            "Epoch 3/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.3733 - val_loss: 0.3332\n",
            "Epoch 4/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.3256 - val_loss: 0.2690\n",
            "Epoch 5/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.2423 - val_loss: 0.2224\n",
            "Epoch 6/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.1714 - val_loss: 0.2703\n",
            "Epoch 7/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1625 - val_loss: 0.2806\n",
            "Epoch 8/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1436 - val_loss: 0.3186\n",
            "Epoch 9/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1007 - val_loss: 0.1951\n",
            "Epoch 10/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0977 - val_loss: 0.2702\n",
            "Epoch 11/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0732 - val_loss: 0.2175\n",
            "Epoch 12/50\n",
            "25/25 [==============================] - 1s 27ms/step - loss: 0.1772 - val_loss: 0.2255\n",
            "Epoch 13/50\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0669 - val_loss: 0.3063\n",
            "Epoch 14/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1402 - val_loss: 0.2856\n",
            "Epoch 15/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0691 - val_loss: 0.2973\n",
            "Epoch 16/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0817 - val_loss: 0.6316\n",
            "Epoch 17/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1267 - val_loss: 0.3448\n",
            "Epoch 18/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1155 - val_loss: 0.2470\n",
            "Epoch 19/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0553 - val_loss: 0.2071\n",
            "Epoch 20/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0882 - val_loss: 0.2395\n",
            "Epoch 21/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0464 - val_loss: 0.3634\n",
            "Epoch 22/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1257 - val_loss: 0.4139\n",
            "Epoch 23/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1080 - val_loss: 0.3234\n",
            "Epoch 24/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0469 - val_loss: 0.2971\n",
            "Epoch 25/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0690 - val_loss: 0.2414\n",
            "Epoch 26/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0502 - val_loss: 0.3879\n",
            "Epoch 27/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0574 - val_loss: 0.3431\n",
            "Epoch 28/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0813 - val_loss: 0.4010\n",
            "Epoch 29/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0944 - val_loss: 0.4872\n",
            "Epoch 30/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0926 - val_loss: 0.2677\n",
            "Epoch 31/50\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0911 - val_loss: 0.4110\n",
            "Epoch 32/50\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.0460 - val_loss: 0.2774\n",
            "Epoch 33/50\n",
            "25/25 [==============================] - 1s 24ms/step - loss: 0.0560 - val_loss: 0.5786\n",
            "Epoch 34/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.1071 - val_loss: 0.4297\n",
            "Epoch 35/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0367 - val_loss: 0.4476\n",
            "Epoch 36/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0661 - val_loss: 0.3022\n",
            "Epoch 37/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0448 - val_loss: 0.4308\n",
            "Epoch 38/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0769 - val_loss: 0.3597\n",
            "Epoch 39/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0513 - val_loss: 0.5548\n",
            "Epoch 40/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0663 - val_loss: 0.3917\n",
            "Epoch 41/50\n",
            "25/25 [==============================] - 1s 28ms/step - loss: 0.0877 - val_loss: 0.4091\n",
            "Epoch 42/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0233 - val_loss: 0.5054\n",
            "Epoch 43/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0830 - val_loss: 0.4251\n",
            "Epoch 44/50\n",
            "25/25 [==============================] - 1s 26ms/step - loss: 0.0699 - val_loss: 0.3184\n",
            "Epoch 45/50\n",
            "25/25 [==============================] - 1s 25ms/step - loss: 0.1073 - val_loss: 0.4423\n",
            "Epoch 46/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0784 - val_loss: 0.5067\n",
            "Epoch 47/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.1368 - val_loss: 0.4503\n",
            "Epoch 48/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.0377 - val_loss: 0.3674\n",
            "Epoch 49/50\n",
            "25/25 [==============================] - 1s 22ms/step - loss: 0.0914 - val_loss: 0.5504\n",
            "Epoch 50/50\n",
            "25/25 [==============================] - 1s 23ms/step - loss: 0.1130 - val_loss: 0.4016\n",
            "7/7 [==============================] - 0s 6ms/step - loss: 0.4016\n",
            "Accuracy: 0.96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6c1c26d0f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8qfeXZenZ7N"
      },
      "source": [
        "## Further information\r\n",
        "For ideas about hyper-parameter tuning, take a look at the strategies described in the sklearn documentation under [model selection](https://scikit-learn.org/stable/model_selection.html), or in this [blog post](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html) from TensorFlow. For a more thorough discussion about optimizers see [this video](https://www.youtube.com/watch?v=DiNzQP7kK-s) discussing the article [Descending through a Crowded Valley -- Benchmarking Deep Learning Optimizers](https://arxiv.org/abs/2007.01547).\r\n",
        "\r\n",
        "\r\n",
        "**Good luck!**"
      ]
    }
  ]
}